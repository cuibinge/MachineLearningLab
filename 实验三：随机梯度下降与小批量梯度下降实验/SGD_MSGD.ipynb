{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 实验三：随机梯度下降（SGD）与小批量梯度下降（Mini-batch SGD）的理解与实现\n",
    "\n",
    "在深度学习与机器学习模型的训练过程中，优化算法起着至关重要的作用。梯度下降法（Gradient Descent）作为最基本也是最常用的优化方法之一，其变体在实际应用中更具效率与灵活性。\n",
    "\n",
    "本实验聚焦于两种常见的梯度下降策略：**随机梯度下降（Stochastic Gradient Descent, SGD）** 和 **小批量梯度下降（Mini-batch SGD）**。二者都是基于传统的**批量梯度下降（Batch Gradient Descent）**进行改进，以提高训练效率和模型收敛的稳定性。\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 背景原理\n",
    "\n",
    "在监督学习中，我们的目标是最小化损失函数 $J(\\theta) $，形式如下：\n",
    "\n",
    "$\n",
    "J(\\theta) = \\frac{1}{n} \\sum_{i=1}^n \\mathcal{L}(f(x_i; \\theta), y_i)\n",
    "$\n",
    "\n",
    "其中：\n",
    "- $ \\mathcal{L} $：损失函数（如 MSE、交叉熵等）  \n",
    "- $ \\theta $：模型参数  \n",
    "- $ n $：样本数量\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 梯度下降法回顾\n",
    "\n",
    "传统的梯度下降更新方式为：\n",
    "\n",
    "$\n",
    "\\theta := \\theta - \\eta \\cdot \\nabla_\\theta J(\\theta)\n",
    "$\n",
    "\n",
    "- 优点：更新方向准确，收敛稳定  \n",
    "- 缺点：每轮更新都需遍历所有样本，计算代价高，不适合大数据场景\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 随机梯度下降（SGD）\n",
    "\n",
    "SGD 每次使用一个样本进行参数更新：\n",
    "\n",
    "$\n",
    "\\theta := \\theta - \\eta \\cdot \\nabla_\\theta \\mathcal{L}(f(x_i; \\theta), y_i)\n",
    "$\n",
    "\n",
    "- 优点：计算效率高，能跳出局部最优  \n",
    "- 缺点：梯度波动大，收敛路径不稳定\n",
    "\n",
    "---\n",
    "\n",
    "## 4. 小批量梯度下降（Mini-batch SGD）\n",
    "\n",
    "Mini-batch SGD 使用一个小样本集合 $ B \\subset \\{1, \\dots, n\\} $：\n",
    "\n",
    "$\n",
    "\\theta := \\theta - \\eta \\cdot \\frac{1}{|B|} \\sum_{i \\in B} \\nabla_\\theta \\mathcal{L}(f(x_i; \\theta), y_i)\n",
    "$\n",
    "\n",
    "- 优点：在收敛稳定性与计算效率之间取得平衡，是深度学习中最常用的优化方式\n",
    "\n",
    "---\n",
    "\n",
    "## 5. 实验目的\n",
    "\n",
    "- 理解 SGD 与 Mini-batch SGD 的基本原理与数学表达  \n",
    "- 学会使用 Python 实现两种方法  \n",
    "- 对比两者在训练过程中的表现差异，如收敛速度、损失波动等\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现小批量梯度下降算法对线性回归求解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "类说明：LinearRegression_msgd\n",
    "    编写代码实现利用小批量梯度下降算法对线性回归求解：\n",
    "    1. 随机选出B条训练数据\n",
    "    2. 计算选出B条训练数据上的经验损失梯度\n",
    "    3. 更新 w 的值\n",
    "    \n",
    "Parameters:\n",
    "      X   - 数据\n",
    "      y   - 标签\n",
    "    eat_0 - 算法参数\n",
    "    eta_1 - 算法参数\n",
    "      N   - 搜索步数\n",
    "      B   - 批次大小\n",
    "Returns:\n",
    "\"\"\"\n",
    "class LinearRegression_msgd:    \n",
    "    \n",
    "    def fit(self, X, y, eta_0 = 10, eta_1 = 50, N = 3000, B = 10):\n",
    "#####  Start Code Here  #####\n",
    "        self.N = N\n",
    "    \n",
    "        # 用于绘图\n",
    "        self.W = np.zeros((N,1))\n",
    "        \n",
    "        # 获取 X 的维度\n",
    "           \n",
    "            \n",
    "        # 初始化权重\n",
    "\n",
    "        \n",
    "        # 开始 N 轮循环, 进行梯度下降搜索\n",
    "        for t in range(N):\n",
    "\n",
    "            \n",
    "            self.W[t][0] = np.sum(w[0])\n",
    "            \n",
    "        self.w /= N\n",
    "#####  End Code Here  #####    \n",
    "    def predict(self, X):\n",
    "        return X.dot(self.w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 实现随机梯度下降算法对线性回归求解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "类说明：LinearRegression\n",
    "    编写代码实现利随机梯度下降算法对线性回归求解：\n",
    "    1. 从训练数据中进行随机采样\n",
    "    2. 计算经验损失的梯度\n",
    "    3. 更新 w 的值\n",
    "Parameters:\n",
    "      X   - 数据\n",
    "      y   - 标签\n",
    "    eat_0 - 算法参数\n",
    "    eta_1 - 算法参数\n",
    "      N   - 搜索步数\n",
    "Returns:\n",
    "\"\"\"\n",
    "class LinearRegression:\n",
    "\n",
    "    def fit(self, X, y, eta_0 = 10, eta_1 = 50, N = 3000):\n",
    "#####  Start Code Here  #####\n",
    "        # 获取X的维度\n",
    "        \n",
    "        \n",
    "        # 用于绘图\n",
    "        self.W = np.zeros((N, 1))\n",
    "        \n",
    "        # 初始化权重 w\n",
    "       \n",
    "    \n",
    "        # 开始 N 轮循环, 进行梯度下降搜索\n",
    "        for t in range(N):\n",
    "            \n",
    "            \n",
    "            self.W[t][0] = np.sum(w[0])\n",
    "            \n",
    "\n",
    "#####  End Code Here  #####  \n",
    "        self.w /= N\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return X.dot(self.w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【实验实战】：飞机噪音"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用一个来自 NASA 的测试不同飞机机翼噪音的数据集来比较随机梯度下降算法和小批量梯度下降算法， 我们使用该数据集的前 1500 个样本和 5 个特征， 并使用标准化对数据进行预处理。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_ch7():  \n",
    "    data = np.genfromtxt('airfoil_self_noise.dat', delimiter = '\\t')\n",
    "    data = (data - data.mean(axis = 0)) / data.std(axis = 0)\n",
    "    return np.array(data[:1300, :-1]), np.array(data[:1300, -1]), np.array(data[1300:1500, :-1]), np.array(data[1300:1500, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取训练集合测试\n",
    "X_train, y_train,X_test, y_test  = get_data_ch7()\n",
    "\n",
    "#####  Start Code Here  #####\n",
    "# 使用 process_features函数对数据进行标准化处理\n",
    "\n",
    "\n",
    "# 定义随机梯度下降算法模型\n",
    "\n",
    "\n",
    "# 模型训练\n",
    "\n",
    "\n",
    "W = model.W\n",
    "fig = plt.figure(figsize = (20,8),dpi = 80)\n",
    "y = np.linspace(1,model.N, model.N)\n",
    "plt.plot(y, W)\n",
    "plt.show()\n",
    "\n",
    "# 计算均方误差和r2系数\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "score = r2_score(y_test, y_pred)\n",
    "print(\"mse = {}, r2 = {}\".format(mse, score))\n",
    "\n",
    "# 定义小批量梯度下降算法模型\n",
    "\n",
    "\n",
    "# 模型训练\n",
    "\n",
    "\n",
    "#####  End Code Here  #####\n",
    "W = model.W\n",
    "fig = plt.figure(figsize = (20,8), dpi = 80)\n",
    "y = np.linspace(1,model.N, model.N)\n",
    "plt.plot(y, W)\n",
    "plt.show()\n",
    "\n",
    "# 计算均方误差和r2系数\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "score = r2_score(y_test, y_pred)\n",
    "print(\"mse = {}, r2 = {}\".format(mse, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d6be7ea",
   "metadata": {},
   "source": [
    "## 实验扩展1：损失函数收敛曲线可视化\n",
    "\n",
    "为了更直观地比较两种优化算法的收敛速度，我们记录每一轮训练过程中的损失值，并绘制损失函数曲线。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11596a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 假设你在模型训练过程中记录了每一轮的损失\n",
    "# 这里模拟两个损失序列（真实使用时请从模型中提取）\n",
    "epochs = list(range(1, 101))\n",
    "loss_sgd = [np.exp(-0.05 * x) + 0.02 * np.random.randn() for x in epochs]\n",
    "loss_msgd = [np.exp(-0.08 * x) + 0.02 * np.random.randn() for x in epochs]\n",
    "\n",
    "plt.plot(epochs, loss_sgd, label='SGD')\n",
    "plt.plot(epochs, loss_msgd, label='Mini-batch SGD')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('损失函数收敛曲线')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca9073c",
   "metadata": {},
   "source": [
    "## 实验扩展2：不同 Batch Size 下 Mini-batch SGD 的表现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d033e6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_sizes = [1, 16, 32, 64]\n",
    "# 假设我们已有一个训练函数 train_msgd(batch_size)，返回最终误差\n",
    "results = {}\n",
    "\n",
    "for b in batch_sizes:\n",
    "    # 此处示例使用模拟误差，真实实验中请调用模型训练过程\n",
    "    error = np.random.uniform(0.02, 0.1)  # 替换为真实误差\n",
    "    results[b] = error\n",
    "\n",
    "# 绘图展示误差随 batch size 变化趋势\n",
    "plt.plot(results.keys(), results.values(), marker='o')\n",
    "plt.xlabel('Batch Size')\n",
    "plt.ylabel('Final MSE')\n",
    "plt.title('不同 Batch Size 的模型误差')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbe13ac",
   "metadata": {},
   "source": [
    "## 实验扩展3：测试集性能对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08adaaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 假设两个模型的测试误差如下（请用真实误差替换）\n",
    "mse_sgd = 0.045\n",
    "mse_msgd = 0.032\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame({\n",
    "    '算法': ['SGD', 'Mini-batch SGD'],\n",
    "    '测试集MSE': [mse_sgd, mse_msgd]\n",
    "})\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3222d5",
   "metadata": {},
   "source": [
    "\n",
    "## 实验总结\n",
    "\n",
    "- **SGD** 收敛速度较慢，但实现简单，占用内存小。\n",
    "- **Mini-batch SGD** 能在收敛速度和计算效率之间取得平衡，适合大规模数据训练。\n",
    "- 不同 batch size 会影响训练稳定性和误差大小，需根据具体任务选择合适的值。\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e42634819b8c191a5d07eaf23810ff32516dd8d3875f28ec3e488928fbd3c187"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
